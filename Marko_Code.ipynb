{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86fd1e03",
   "metadata": {},
   "source": [
    "# This is one of my backups - I'm struggling to find the final version I had, could be that all copies were in places associated with my Cranfield credentials.\n",
    "\n",
    "*The functions are all present and correct, but its not my tidiest work - I could probably spend some more time on sorting that out when I take some holiday, an I'm happy to answer questions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d024be23",
   "metadata": {
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1630850707344,
     "user": {
      "displayName": "Marko Stojanovic",
      "photoUrl": "",
      "userId": "02163421072509415080"
     },
     "user_tz": -60
    },
    "id": "d024be23"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "from scipy.stats import t as tdstr\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import zscore\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import savgol_filter\n",
    "from datetime import date, datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib\n",
    "matplotlib.rc('font', size=14)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.matlib as npm \n",
    "%matplotlib inline\n",
    "from numpy.fft import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1402c762",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1630850708622,
     "user": {
      "displayName": "Marko Stojanovic",
      "photoUrl": "",
      "userId": "02163421072509415080"
     },
     "user_tz": -60
    },
    "id": "385645a7"
   },
   "source": [
    "# Denoising MODIS data using FFT \n",
    "![Denoising Algorithm](img/picture.png)\n",
    "\n",
    "This is something I came up with for this project to retain enough data points to use ***It has not been that thouroughly tested or \"approved\" as a method for cleaning an NDVI time series***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dc1605b1",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1630850708623,
     "user": {
      "displayName": "Marko Stojanovic",
      "photoUrl": "",
      "userId": "02163421072509415080"
     },
     "user_tz": -60
    },
    "id": "dc1605b1"
   },
   "outputs": [],
   "source": [
    "# denoise using FFT, if not using daily data, threshold should be multiplied by the sampling interval \n",
    "\n",
    "def ff_denoise(signal, threshold=2):\n",
    "    fourier = rfft(signal)\n",
    "    \n",
    "    frequencies = rfftfreq(signal.size,d=1/365.25)\n",
    "    \n",
    "    \n",
    "    fourier[frequencies > threshold] = 0\n",
    "    \n",
    "    return irfft(fourier,len(signal))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###------THE PLAN-----####\n",
    "\n",
    "# 1.take an NDVI time series (NDVI) and its sampling frequency (gap)\n",
    "# 2.interpolate across missing values (otherwise the fft just gives us NaN+NaN i for everything)\n",
    "# 3.apply fft_denoise() (frequency = 2 cycles per year (frequency*gap))\n",
    "# 4.drop everything more than \"pad\" below the curve\n",
    "\n",
    "# 5. repeat steps 2-4 for a set number of iterations, or until no values are dropped, whichever comes first\n",
    "# Note: currently we're dropping and reinterpolating all previously dropped values\n",
    "\n",
    "###---INPUTS-----####\n",
    "\n",
    "# NDVI : NDVI time series\n",
    "# GAP : sampling frequency of the NDVI time series\n",
    "# FREQUENCY : threshold frequency that is used to determine high frequency variation due to noise/clouds\n",
    "# PAD : Threshold below the smoothed waveform - values < smooth wave - pad are dropped at each iteration\n",
    "# iterations : Max number of iterations - will stop early if no values are dropped in consecutive iterations\n",
    "\n",
    "\n",
    "\n",
    "def ff_iterdenoise(NDVI, gap=1, frequency=2, pad=0.1,iterations=10):\n",
    "    \n",
    "    # want to take a series or array, return a DF with filtered results +keep originals\n",
    "    s_NDVI=pd.DataFrame({\"raw\":NDVI,\"filtered\":NDVI})\n",
    "    \n",
    "    # c is where we're going to keep track of the filtered NDVI values, as well as on which iteration they were dropped\n",
    "    s_NDVI[\"c\"]=-1\n",
    "    s_NDVI.loc[s_NDVI.raw.isna(),\"c\"]=0 # already missing values\n",
    "    \n",
    "    # this is a bit dodgey, but what we're doing is appending the count of '-1's in c, and looping until it doesn't change\n",
    "    li=[-1,-10]  \n",
    "    i=0\n",
    "    while li[i] != li[i+1] and i<iterations:\n",
    "        \n",
    "        # interpolating\n",
    "        Y = s_NDVI['filtered'].values\n",
    "        x =s_NDVI.index.copy()\n",
    "\n",
    "        indices = ~np.isnan(Y)\n",
    "\n",
    "        yinterp = np.interp(x[~indices], x[indices], Y[indices])\n",
    "        Y[~indices] = yinterp\n",
    "        \n",
    "        # applying the ff filter from earlier, calculating \"error\"\n",
    "        \n",
    "        ff=ff_denoise(s_NDVI.filtered,frequency*gap)\n",
    "\n",
    "        s_NDVI[\"ff\"]=ff\n",
    "\n",
    "        s_NDVI[\"delta\"]=s_NDVI.ff-s_NDVI.filtered\n",
    "\n",
    "\n",
    "        s_NDVI.loc[s_NDVI.delta>pad,\"c\"]=i+1  # these are the values below our threshold\n",
    "\n",
    "##---can uncomment this and plot each time if we want to ----##         \n",
    "\n",
    "#         fig = plt.figure(figsize=(16, 6))\n",
    "#         ax1 = plt.subplot(111)\n",
    "#         ax1.plot(s_NDVI.index,s_NDVI.ff,\"-\", c='k')\n",
    "#         s=ax1.scatter(s_NDVI.index,s_NDVI.filtered, c=s_NDVI.c, cmap=\"Set1\",s=10)\n",
    "# #        ax1.plot(sixteen_d, ls=\"--\", marker=\"x\",c=\"b\")\n",
    "#         fig.colorbar(s)\n",
    "#         ax1.set_ylabel('NDVI')\n",
    "#         ax1.set_ylim([-0.1,1])\n",
    "#         ax1.set_xlim([datetime(2016,1,1),datetime(2020,1,1)])  # <- to view a smaller time period\n",
    "\n",
    "\n",
    "        s_NDVI.loc[s_NDVI.c!=-1,\"filtered\"]=np.nan \n",
    "\n",
    "        li.append(s_NDVI.c.value_counts()[-1])\n",
    "        i+=1\n",
    "        \n",
    "    s_NDVI.drop(\"delta\",axis=1, inplace=True)    #don't need this\n",
    "    return s_NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a862ca42",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1630850708247,
     "user": {
      "displayName": "Marko Stojanovic",
      "photoUrl": "",
      "userId": "02163421072509415080"
     },
     "user_tz": -60
    },
    "id": "a862ca42"
   },
   "outputs": [],
   "source": [
    "## We need a run of consecutive NDVI values at the start or the model fails ##\n",
    "# this lets us identify the longest run in the first \"days\" days so we can use it as a start point\n",
    "\n",
    "def get_con(NDVI,days=365):\n",
    "    a = NDVI[:days]  \n",
    "    m = np.concatenate(( [True], np.isnan(a), [True] ))  # Mask\n",
    "    ss = np.flatnonzero(m[1:] != m[:-1]).reshape(-1,2)   # Start-stop limits\n",
    "    start,stop = ss[(ss[:,1] - ss[:,0]).argmax()]\n",
    "    return a.index[start],stop-start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20bea97",
   "metadata": {
    "id": "d20bea97"
   },
   "source": [
    "# These are the functions from the original paper\n",
    "\n",
    "- I've added the \"gap\" paramater where necessary so that we can work with daily MODIS data vs just the 16 day Landsat they used\n",
    "\n",
    "- I've also changed the way \"delta\" (the discount factor works) we're doing component discounting rather than using a single discount factor like the California paper did (some discussion about why in my write up)\n",
    "\n",
    "- I've fixed \"delvar\" Dan - we briefly discussed this over zoom at one point, I'm convinced the way this was originally implemented is incorrect - discounting the degrees of freedom the way it is implemented in the original code does not do what they say it does\n",
    "\n",
    "- The commented out lines of code were used to plot mortality and droughts etc in the original paper. I didn't use them, but I've kept them there because they were in the original\n",
    "\n",
    "- The rest of the comments are from the original author(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe7c29d",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1630850707346,
     "user": {
      "displayName": "Marko Stojanovic",
      "photoUrl": "",
      "userId": "02163421072509415080"
     },
     "user_tz": -60
    },
    "id": "bbe7c29d"
   },
   "outputs": [],
   "source": [
    "class Prior:\n",
    "    def __init__(self, m, C, S, nu): \n",
    "        self.m = m # mean of t-distribution \n",
    "        self.C = C # scale matrix of t-distribution\n",
    "        self.S = S # precision ~ IG(nu/2,S*nu/2)\n",
    "        self.nu = nu # degree of freedom\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,Y,X,rseas,delta):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.rseas = rseas\n",
    "        #dd = np.ones(4)*delta   # <- MARKO CHANGED THIS BIT\n",
    "        self.delta = delta\n",
    "        ntrend = 2;nregn = X.shape[1]; pseas = len(rseas);nseas = pseas*2;\n",
    "        m = np.zeros([ntrend+nregn+nseas,1])\n",
    "        C = scipy.linalg.block_diag(1*np.eye(ntrend),1*np.eye(nregn),1*np.eye(nseas))\n",
    "        S = np.power(0.2,2); nu = ntrend+nregn+pseas;\n",
    "        pr = Prior(m,C,S,nu)\n",
    "        self.prior = pr\n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "def forwardFilteringM(Model,gap):\n",
    "# All the parameters estimated here correspond Eqs. 13-16 and the related ones in the Supplementary Information of Liu et al. (2019)\n",
    "# notation in the code -> notation in Liu et al., 2019: \n",
    "# m -> m_t; C -> C_t^{**}; nu -> n_t; \n",
    "# a -> a_t; R -> R_t^{**}; F -> F_t; e -> e_t; y -> y_t; Q -> q_t^{**}; f -> f_t; S -> s_t = d_t/n_t\n",
    "\n",
    "  Y = Model.Y\n",
    "  X = Model.X\n",
    "  rseas = Model.rseas\n",
    "  delta = Model.delta\n",
    "  Prior = Model.prior\n",
    "  period = 365.25/gap\n",
    "  deltrend = delta[0];delregn = delta[1];delseas = delta[2];delvar = 0.999\n",
    "  \n",
    "  Ftrend = np.array([[1],[0]]);ntrend = len(Ftrend); Gtrend = np.array([[1,1],[0,1]]);itrend = np.arange(0,ntrend)\n",
    "  nregn = X.shape[1];Fregn = np.zeros([nregn,1]);Gregn=np.eye(nregn);iregn = np.arange(ntrend,ntrend+nregn)\n",
    "  pseas = len(rseas);nseas = pseas*2;iseas = np.arange(ntrend+nregn,ntrend+nregn+nseas)\n",
    "  Fseas = npm.repmat([[1],[0]],pseas,1);Gseas = np.zeros([nseas,nseas]);\n",
    "  for j in range(pseas):\n",
    "      c = np.cos(2*np.pi*rseas[j]/period);\n",
    "      s = np.sin(2*np.pi*rseas[j]/period);\n",
    "      i = np.arange(2*j,2*(j+1))\n",
    "      Gseas[np.reshape(i,[2,1]),i] = [[c,s],[-s,c]]\n",
    "  F = np.concatenate((Ftrend,Fregn,Fseas),axis=0)\n",
    "  G = scipy.linalg.block_diag(Gtrend,Gregn,Gseas) \n",
    "  m = Prior.m; C = Prior.C; S = Prior.S; nu = Prior.nu\n",
    "\n",
    "  T = len(Y)\n",
    "  sm = np.zeros(m.shape)\n",
    "  sC = np.zeros([C.shape[0],C.shape[1],1])\n",
    "  sS = np.zeros(1)\n",
    "  snu = np.zeros(1)\n",
    "  slik = np.zeros(1)\n",
    "  \n",
    "  for t in range(T):\n",
    "      a = np.dot(G,m)\n",
    "      R = np.dot(np.dot(G,C),np.transpose(G))\n",
    "      R[np.reshape(itrend,[-1,1]),itrend] = R[np.reshape(itrend,[-1,1]),itrend]/deltrend\n",
    "      R[np.reshape(iregn,[-1,1]),iregn] = R[np.reshape(iregn,[-1,1]),iregn]/delregn\n",
    "      R[np.reshape(iseas,[-1,1]),iseas] = R[np.reshape(iseas,[-1,1]),iseas]/delseas\n",
    "      nu = delvar*nu\n",
    "      F[iregn,0] = X[t,]\n",
    "\n",
    "      A = np.dot(R,F);Q = np.squeeze(np.dot(np.transpose(F),A)+S); A = A/Q; f = np.squeeze(np.dot(np.transpose(F),a))\n",
    "      y = Y[t]\n",
    "\n",
    "      if ~np.isnan(y):\n",
    "          e = y-f; ac = (nu+np.power(e,2)/Q)/(nu+1)\n",
    "          rQ = np.sqrt(Q)\n",
    "          mlik = tdstr.pdf(e/rQ,nu)/rQ\n",
    "          m = a+A*e; C = ac*(R-np.dot(A,np.transpose(A))*Q); nu = nu+1; S = ac*S; \n",
    "          # About \"S = ac*S\" (using the notations in Liu et al. (2019)): \n",
    "          # s_t = d_t/n_t = (d_{t-1}+e_t^2/(q_t^{**}/s_t))/n_t = s_{t-1} * (n_{t-1}+e_t^2/(q_t^{**})/n_t = ac * s_{t-1}\n",
    "      else:\n",
    "          m = a; C = R;\n",
    "          if t<T-1:\n",
    "              X[t+1,0] = f\n",
    "          mlik = np.nan\n",
    "      sm = np.concatenate((sm,m),axis=1)\n",
    "      sC = np.concatenate((sC,np.reshape(C,[C.shape[0],C.shape[1],1])),axis=2)\n",
    "      snu = np.concatenate((snu,[nu]),axis=0)\n",
    "      sS = np.concatenate((sS,[S]),axis=0)\n",
    "      slik = np.concatenate((slik,[mlik]),axis=0)\n",
    "      \n",
    "  return {'sm':sm, 'sC':sC ,'snu':snu,'slik':slik} \n",
    "\n",
    "def computeAnormaly(CLM,AvgCLM,date0,gap):\n",
    "  deltaT = timedelta(days=gap)\n",
    "  anCLM = np.zeros([1,CLM.shape[1]])\n",
    "  for i in range(CLM.shape[0]):\n",
    "    st = date0+deltaT*(i)\n",
    "    st = st.timetuple().tm_yday\n",
    "    et = date0+deltaT*(i+1); et = et.timetuple().tm_yday               \n",
    "    if et<st:\n",
    "        window = np.concatenate((np.arange(st,365),np.arange(0,et)))\n",
    "    else:\n",
    "        window = np.arange(st,et)\n",
    "    window[window==365] = 0  # leap year\n",
    "    anCLM = np.concatenate((anCLM,np.reshape(CLM[i,:]- np.mean(AvgCLM[window,:],axis = 0),[1,CLM.shape[1]])),axis=0)\n",
    "    \n",
    "  return anCLM[1:,:]\n",
    "\n",
    "def Index_low(nn,date0,percentile,gap):\n",
    "  intervel = gap\n",
    "  date0_num = date0.toordinal()\n",
    "  dd = np.arange(date0,date0+timedelta(days=intervel)*len(nn),timedelta(days=intervel))\n",
    "  dd_num = np.arange(date0_num,date0_num+intervel*(len(nn)),intervel)\n",
    "  idq = [i for i in range(len(nn)) if np.isfinite(nn[i])] \n",
    "  tt1_num = np.arange(dd_num[idq[0]],dd_num[idq[-1]],1)\n",
    "  f_itp = interp1d(dd_num[idq], nn[idq],kind = 'linear')\n",
    "  nn_itp = f_itp(tt1_num)\n",
    "\n",
    "  yday = np.array([date.fromordinal(tt1_num[i]).timetuple().tm_yday for i in range(len(tt1_num))])\n",
    "\n",
    "  ndvi_mean = np.array([np.mean(nn_itp[yday==i]) for i in range(1,366)])\n",
    "  ndvi_std = np.array([np.std(nn_itp[yday==i]) for i in range(1,366)])\n",
    "  if len(ndvi_mean)==365:\n",
    "    ndvi_mean = np.concatenate((ndvi_mean,[ndvi_mean[-1]]),axis = 0)\n",
    "    ndvi_std = np.concatenate((ndvi_std,[ndvi_std[-1]]),axis = 0)\n",
    "\n",
    "  tt2 = np.arange(dd[0],dd[-1],timedelta(days=1))\n",
    "  tt2_num = np.arange(dd_num[0],dd_num[-1],1)\n",
    "  yday2 = np.array([date.fromordinal(tt2_num[i]).timetuple().tm_yday for i in range(len(tt2_num))])\n",
    "  nv = norm.ppf(1-(1-percentile)/2)\n",
    "  lowboundary = np.array([ndvi_mean[yday2[i]-1]-nv*ndvi_std[yday2[i]-1] for i in range(len(tt2))])\n",
    "\n",
    "  index_low = [i for i in range(len(dd)) if (~np.isnan(nn[i])) and (nn[i]<lowboundary[tt2_num==dd_num[i]])] \n",
    "  return index_low\n",
    "\n",
    "def PlotEWS(N,date0,sm,sC,snu,gap):\n",
    "  # thresholds for identification of abnormally high autocorrelation (EWS)  \n",
    "  # and abnormally low NDVI(ALN)\n",
    "  quantile1 = 0.95\n",
    "  quantile2 = 0.80 \n",
    "\n",
    "  steps = [date0+relativedelta(days=gap*i) for i in range(len(N))]\n",
    "  lown = Index_low(N,date0,quantile2,gap)\n",
    "  lown_continuous = []\n",
    "  for i in range(len(lown)):\n",
    "    tile = [j for j in lown if (j<=lown[i] and j>=lown[i]-int(90/gap))] \n",
    "    if len(tile)>2: \n",
    "        #NDVI being abnormally low fro more than half of the time within 3 mon\n",
    "        lown_continuous = np.concatenate([lown_continuous,[lown[i]]])\n",
    "        lown_continuous = np.array(lown_continuous).astype(int)\n",
    "  tmp = np.array([steps[i] for i in lown_continuous])\n",
    "  #diebackdate = tmp[0]\n",
    "\n",
    "  steps = np.array(steps)\n",
    "\n",
    "  xpos = date0\n",
    "  xtick = [date0+relativedelta(years=2*i) for i in range(0,9)]\n",
    "\n",
    "\n",
    "  plt.figure(figsize=(16, 12))\n",
    "  ax1 = plt.subplot(211)\n",
    "\n",
    "  xlim = [datetime(date0.year,1,1),datetime(2020,1,1)]\n",
    "  ylim = [0,1]\n",
    "  ax1.plot(steps,N,'o-k',markersize=5,label='NDVI')\n",
    "  ax1.plot(steps[lown_continuous],N[lown_continuous],'or',label='ALN')\n",
    "\n",
    "  # ax1.axvspan(datetime(2007,1,1), datetime(2010,1,1), color='brown', alpha=0.1, lw=0)\n",
    "  # ax1.axvspan(datetime(2011,1,1), datetime(2016,1,1), color='brown', alpha=0.1, lw=0)\n",
    "\n",
    "  #xtick = [date0+relativedelta(years=2*i) for i in range(0,2)]\n",
    "  #ax1.set_xticks(xtick,('00','02','04','06','08','10','12','14','16'))\n",
    "\n",
    "  ax1.set_ylim(ylim)\n",
    "  ax1.set_xlim(xlim)\n",
    "  ax1.set_ylabel('NDVI')\n",
    "  ax1.legend(loc = 'lower left',ncol=2)\n",
    "  # ax1.text(xpos, 0.82, '(a)',fontsize=20)\n",
    "  #ax1.set_xticks(xtick)\n",
    "  plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "\n",
    "  warmup = int(730/gap)\n",
    "  bd = list(map(lambda m,C,nu: m+C*tdstr.ppf(quantile1,nu),sm,sC,snu))\n",
    "  bd2 = list(map(lambda m,C,nu: m+C*tdstr.ppf(quantile2,nu),sm,sC,snu))\n",
    "\n",
    "  mbd = np.median(bd[warmup:])\n",
    "  ews = np.array([i for i,im in enumerate(sm) if im >mbd])\n",
    "  ews = ews[ews>warmup]\n",
    "  ews_continuous = []\n",
    "  window = int(90/gap) # three months\n",
    "\n",
    "  for i in range(len(ews)):\n",
    "    tile = [j for j in ews if (j<=ews[i] and j>=ews[i]-window)]\n",
    "    if len(tile)>window-1:\n",
    "        ews_continuous = np.concatenate([ews_continuous,[ews[i]]])\n",
    "  ews_continuous = np.array(ews_continuous).astype(int)\n",
    "  tmp = steps[ews_continuous]\n",
    "  #ewsdate = tmp[tmp>datetime(2012,7,15)][0]\n",
    "  #mortdate = datetime(2015,7,15)\n",
    "  arrowprops=dict(facecolor='black', shrink=0.05,width=1,headwidth=10)\n",
    "  ax2 = plt.subplot(212)\n",
    "  ylim = [-1,1]\n",
    "  ax2.plot(steps[1:], sm[1:], lw=2, label='mean')\n",
    "  ax2.fill_between(steps, 2*sm-bd, bd, facecolor='0.7',label=str(int(quantile1*100))+'% range')\n",
    "  ax2.fill_between(steps, 2*sm-bd2, bd2, facecolor='0.5',label=str(int(quantile2*100))+'% range')\n",
    "  ax2.plot([steps[warmup],steps[-1]],[mbd,mbd],'--',color='0.4')\n",
    "  ax2.plot(steps[ews_continuous],sm[ews_continuous],'^r',markersize=3,label='EWS')\n",
    "  #  ax2.axvspan(datetime(2007,1,1), datetime(2010,1,1), color='brown', alpha=0.1, lw=0)\n",
    "  #  ax2.axvspan(datetime(2011,1,1), datetime(2016,1,1), color='brown', alpha=0.1, lw=0)\n",
    "  ax2.set_xlim(xlim)\n",
    "  #ax2.set_xticks(xtick)\n",
    "  #ax2.set_xticklabels(('05','07','09','11','13','15','17','19'))\n",
    "  ax2.set_ylim(ylim)\n",
    "  ax2.set_ylabel('Autocorrelation')\n",
    "  ax2.set_xlabel('Year')\n",
    "  yend = -0.28\n",
    "  ft = 14\n",
    "  hshift = relativedelta(months=3)\n",
    "  #     ax2.text(mortdate-hshift, 0.05, 'mortality',rotation='vertical',fontsize=ft)\n",
    "  #     ax2.text(diebackdate-hshift, 0.00, 'dieback',rotation='vertical',fontsize=ft)\n",
    "  #ax2.text(ewsdate-hshift, -0.12, 'EWS',rotation='vertical',fontsize=ft)\n",
    "\n",
    "  #     ax2.annotate('', xy=(mortdate, ylim[0]), \n",
    "  #                 xytext=(mortdate, yend),arrowprops=arrowprops)\n",
    "  #     ax2.annotate('', xy=(diebackdate, ylim[0]), \n",
    "  #                 xytext=(diebackdate, yend),arrowprops=arrowprops)\n",
    "  #     ax2.annotate('', xy=(ewsdate, ylim[0]), \n",
    "  #                 xytext=(ewsdate, yend),arrowprops=arrowprops)\n",
    "  ax2.legend(loc=9,ncol=2)\n",
    "  #     ax2.text(xpos, 0.63, '(b)',fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f972db7e",
   "metadata": {},
   "source": [
    "# This is my attempt at determining appropriate values for the discount factors - it takes a really long time. \n",
    "\n",
    "We're ranking based on likelihood as per recommendations in the literature, but the forward filtering calculates residuals at each step (one could also use MAE or RMSE or something similar - when running forwardFilteringM you'd just need an extra line to write the error *e* (=Y-f) to an array and return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f0dfea0e",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1630850708246,
     "user": {
      "displayName": "Marko Stojanovic",
      "photoUrl": "",
      "userId": "02163421072509415080"
     },
     "user_tz": -60
    },
    "id": "f0dfea0e"
   },
   "outputs": [],
   "source": [
    "def fit_delta(mod,gap,s=0.98):\n",
    "    start=s\n",
    "    end=0.9999\n",
    "\n",
    "    strend = start\n",
    "    etrend = end\n",
    "    \n",
    "    sregn = start\n",
    "    eregn = end\n",
    "    \n",
    "    sseas = start\n",
    "    eseas = end\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for b in [0.005,0.001]:\n",
    "        trend=np.arange(strend,min(etrend,0.9999),b)\n",
    "        regn=np.arange(sregn,min(end,eregn),b)\n",
    "        seas=np.arange(sseas,min(eseas,end),b)\n",
    "        \n",
    "\n",
    "        poss_deltas=np.array(np.meshgrid(trend, regn, seas)).T.reshape(-1,3)\n",
    "\n",
    "        poss_deltas=poss_deltas[np.all(poss_deltas<=1,axis=1)]\n",
    "\n",
    "        \n",
    "        \n",
    "        like=[]\n",
    "        \n",
    "        for smurf in np.arange(0,poss_deltas.shape[0]): \n",
    "          deltas = poss_deltas[smurf]\n",
    "          \n",
    "          M=Model(mod.Y,mod.X,mod.rseas,deltas)\n",
    "          f = forwardFilteringM(M,gap)\n",
    "         \n",
    "          like.append(f[\"slik\"])\n",
    "\n",
    "\n",
    "\n",
    "        loglike=[]\n",
    "        for arr in like:\n",
    "            loglike.append(np.nansum(np.log(arr[int(730/gap):])))\n",
    "        \n",
    "        \n",
    "\n",
    "        score=np.zeros(poss_deltas.shape[0])\n",
    "\n",
    "        for i in range(poss_deltas.shape[0]):\n",
    "            \n",
    "            score[np.argsort(loglike)[i]] +=poss_deltas.shape[0]-i\n",
    "\n",
    "        rank=poss_deltas[np.argsort(score)]\n",
    "        \n",
    "\n",
    "        trend=np.arange(strend,etrend,b)\n",
    "        regn=np.arange(sregn,eregn,b)\n",
    "        seas=np.arange(sseas,eseas,b)\n",
    "        \n",
    "\n",
    "\n",
    "        strend = rank[0][0]-b\n",
    "        etrend = rank[0][0]+b\n",
    "        \n",
    "        sregn = rank[0][1]-b\n",
    "        eregn = rank[0][1]+b\n",
    "        \n",
    "        sseas = rank[0][2]-b\n",
    "        eseas = rank[0][2]+b\n",
    "        \n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "        print(rank[:5],np.sort(score)[-5:])\n",
    "    \n",
    "    return rank[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fc2eeb8c",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1630850708621,
     "user": {
      "displayName": "Marko Stojanovic",
      "photoUrl": "",
      "userId": "02163421072509415080"
     },
     "user_tz": -60
    },
    "id": "fc2eeb8c"
   },
   "outputs": [],
   "source": [
    "## this is what I wase using to run things quickly ##\n",
    "# NDVI is a series of filtered NDVI values\n",
    "# weather is our (normalised) weather anomalies\n",
    "# gap is obs frequency of the sat data\n",
    "# delta is the 3 delta values to use - the ones i came up with are in my thesis\n",
    "# if fit then we're running the delta finding function and using those values (with lower bound for the deltas as s)\n",
    "# 0.98 is good for the daily stuff, \n",
    "# i'd leave a wider range for less frequent data, maybe adjust \"b\" in that function to search a bit more coarsely\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_paper(NDVI,weather,gap=1,delta=[1,1,1],fit=False,s=0.98): \n",
    "    \n",
    "    N = NDVI.values\n",
    "    fill_value = -999\n",
    "\n",
    "    CLM = weather.values\n",
    "    AvgCLM = ave_weather.values\n",
    "\n",
    "\n",
    "    date0 = weather.index[0]\n",
    "\n",
    "    anCLM = weather.values\n",
    "\n",
    "    # center NDVI time series\n",
    "    N[N==fill_value] = np.nan\n",
    "    Y = N[1:]-np.nanmean(N) \n",
    "\n",
    "    # use two seasonal harmonic components\n",
    "    rseas = [1,2] \n",
    "\n",
    "    # include lag-1 centerred NDVI and precipitation in the regression module \n",
    "    X = np.column_stack((N[:-1]-np.nanmean(N),anCLM[:-1,])) \n",
    "\n",
    "    # set up model and run forward filtering\n",
    "    delta = delta\n",
    "    M = Model(Y,X,rseas,delta)\n",
    "    \n",
    "    if fit:\n",
    "        delta = fit_delta(M,gap,s) \n",
    "    \n",
    "    M = Model(Y,X,rseas,delta)\n",
    "    FF = forwardFilteringM(M,gap)\n",
    "\n",
    "    # model likelihood\n",
    "    slik = FF.get('slik')\n",
    "\n",
    "    # extract estimates on the coefficient corresponding to lag-1 NDVI\n",
    "    vid = 2 # index of autocorrelation\n",
    "    sm = FF.get('sm')[vid,:] # mean of autocorrelation\n",
    "    sC = FF.get('sC')[vid,vid,:] # variance of autocorrelation\n",
    "    snu = FF.get('snu') # degree of freedom\n",
    "\n",
    "    # plot Fig. 1 in the manuscript\n",
    "    #ax=PlotEWS(N,date0,sm,sC,snu,gap)\n",
    "    return FF,delta,date0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0da2a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1630850708625,
     "user": {
      "displayName": "Marko Stojanovic",
      "photoUrl": "",
      "userId": "02163421072509415080"
     },
     "user_tz": -60
    },
    "id": "63uQUITYj8aA",
    "outputId": "7b9cbbb2-0d17-466f-963e-49955d21620e"
   },
   "outputs": [],
   "source": [
    "###DONT RUN THE NEXT CELL IF YOU WANT TO KEEP THIS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9da9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935662ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a2c262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "63uQUITYj8aA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1630850708625,
     "user": {
      "displayName": "Marko Stojanovic",
      "photoUrl": "",
      "userId": "02163421072509415080"
     },
     "user_tz": -60
    },
    "id": "63uQUITYj8aA",
    "outputId": "7b9cbbb2-0d17-466f-963e-49955d21620e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Map_no</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>SHAPE_Area</th>\n",
       "      <th>ArableRestorationSites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.0</td>\n",
       "      <td>Stonehenge_2005</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>51.178922</td>\n",
       "      <td>-1.838294</td>\n",
       "      <td>49.99</td>\n",
       "      <td>Stonehenge_2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.0</td>\n",
       "      <td>Stonehenge_2007_2016</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>51.176233</td>\n",
       "      <td>-1.844025</td>\n",
       "      <td>38.40</td>\n",
       "      <td>Stonehenge_2007_2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.0</td>\n",
       "      <td>Stonehenge_2010</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>51.186129</td>\n",
       "      <td>-1.848284</td>\n",
       "      <td>30.64</td>\n",
       "      <td>Stonehenge_2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Stonehenge_2009</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>51.174443</td>\n",
       "      <td>-1.851183</td>\n",
       "      <td>25.59</td>\n",
       "      <td>Stonehenge_2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.0</td>\n",
       "      <td>Stonehenge_2011</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>51.191528</td>\n",
       "      <td>-1.851128</td>\n",
       "      <td>25.55</td>\n",
       "      <td>Stonehenge_2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.0</td>\n",
       "      <td>Stonehenge_2002_2017</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>51.187837</td>\n",
       "      <td>-1.789613</td>\n",
       "      <td>17.27</td>\n",
       "      <td>Stonehenge_2002_2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.0</td>\n",
       "      <td>Stonehenge_2000_2017a</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>51.180671</td>\n",
       "      <td>-1.805383</td>\n",
       "      <td>16.39</td>\n",
       "      <td>Stonehenge_2000_2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Map_no                   Name  ...  SHAPE_Area  ArableRestorationSites\n",
       "0    23.0        Stonehenge_2005  ...       49.99         Stonehenge_2005\n",
       "1    22.0   Stonehenge_2007_2016  ...       38.40    Stonehenge_2007_2016\n",
       "2    25.0        Stonehenge_2010  ...       30.64         Stonehenge_2010\n",
       "3    21.0        Stonehenge_2009  ...       25.59         Stonehenge_2009\n",
       "4    26.0        Stonehenge_2011  ...       25.55         Stonehenge_2011\n",
       "5    24.0   Stonehenge_2002_2017  ...       17.27    Stonehenge_2002_2017\n",
       "6    18.0  Stonehenge_2000_2017a  ...       16.39    Stonehenge_2000_2017\n",
       "7     NaN                    NaN  ...         NaN                     NaN\n",
       "\n",
       "[8 rows x 7 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FRH_K-LVN1oR",
   "metadata": {
    "executionInfo": {
     "elapsed": 181,
     "status": "aborted",
     "timestamp": 1630850709306,
     "user": {
      "displayName": "Marko Stojanovic",
      "photoUrl": "",
      "userId": "02163421072509415080"
     },
     "user_tz": -60
    },
    "id": "FRH_K-LVN1oR"
   },
   "outputs": [],
   "source": [
    "# NDVI - dataframe - need index = date of collection as datetime, NDVI in column called NDVI (or change NDVI.NDVI in filtered) \n",
    "# weather - (daily) weather columns should be \"temp\" and \"percip\", index is datetime again \n",
    "# ave_weather - long term averages - index is day of year\n",
    "#Change the start_date depending on year of establishment\n",
    "# NDVI is always input as a pandas Series with dates as the index\n",
    "\n",
    "# if you're using the 16 day composite there's a bit more to do with the weather - that's commented out\n",
    "start_date=2017\n",
    "\n",
    "NDVI=NDVI[NDVI.index.year>=start_date]\n",
    "weather=weather[weather.index.year>=start_date]\n",
    "\n",
    "gap=1 \n",
    "\n",
    "#calculating weather anomalies\n",
    "\n",
    "weather[[\"t_anom\",\"p_anom\"]]=weather[[\"temp\",\"precip\"]]-ave_weather.loc[weather.index.dayofyear].values\n",
    "\n",
    "weather[[\"t_anom\",\"p_anom\"]]=weather[[\"t_anom\",\"p_anom\"]]/np.std(weather[[\"t_anom\",\"p_anom\"]])\n",
    "\n",
    "\n",
    "filtered=ff_iterdenoise(NDVI.NDVI, gap=1, frequency=2, pad=0.1,iterations=15)  #running the filtering function\n",
    "\n",
    "\n",
    "\n",
    "s,g=get_con(filtered.filtered,90) #identifying the run of non-nan values to start at\n",
    "\n",
    "f_filt=filtered.filtered[filtered.index>=s].copy()\n",
    "\n",
    "f_filt=f_filt[f_filt.index<=weather.index[-1]] #in case you have more \n",
    "\n",
    "W=weather[weather.index>=s].copy()\n",
    "\n",
    "prob=datetime(2020,1,1)\n",
    "\n",
    "f_filt=f_filt[f_filt.index<prob]\n",
    "\n",
    "W=W[W.index<prob]\n",
    "\n",
    "W=W.drop([\"temp\",\"precip\"],axis=1)\n",
    "\n",
    "\n",
    "FF, delter,date0 =run_paper(f_filt,W,gap=1,delta=[1,1,1],fit=True,s=0.98)\n",
    "\n",
    "# extract estimates on the coefficient corresponding to lag-1 NDVI\n",
    "vid = 2 # index of autocorrelation\n",
    "sm = FF.get('sm')[vid,:] # mean of autocorrelation\n",
    "sC = FF.get('sC')[vid,vid,:] # variance of autocorrelation\n",
    "snu = FF.get('snu') # degree of freedom\n",
    "\n",
    "PlotEWS(f_filt,date0,sm,sC,snu,gap)\n",
    "\n",
    "#### for the 16 day composite I did this #####\n",
    "\n",
    "# sixteen_d.NDVI is our 16 day ndvi (pandas) time  series in the same form as NDVI above\n",
    "\n",
    "\n",
    "# gap=16\n",
    "#   filt_sd=ff_iterdenoise(sixteen_d.NDVI, gap=16, frequency=2, pad=0.15 ,iterations=12)\n",
    "\n",
    "#   sixteen_d.loc[sixteen_d.NDVI.isna(),'NDVI']=-999\n",
    "\n",
    "#   sd=weather.join(sixteen_d)\n",
    "\n",
    "#   ind=sd[sd.NDVI.isna()==False].index\n",
    "\n",
    "#   for i in np.arange(0,len(ind)-1):\n",
    "#       sd.loc[ind[i],\"heat\"]=sd[ind[i]:ind[i+1]].t_anom.mean()\n",
    "#       sd.loc[ind[i],\"rain\"]=sd[ind[i]:ind[i+1]].p_anom.mean()\n",
    "      \n",
    "#   sd.loc[ind[-1],\"rain\"]=sd[ind[-1]:sd.index[-1]].p_anom.mean()\n",
    "#   sd.loc[ind[-1],\"heat\"]=sd[ind[-1]:sd.index[-1]].t_anom.mean()\n",
    "\n",
    "#   sd_weather=sd[[\"heat\",\"rain\"]].copy().dropna()\n",
    "\n",
    "# FF, delter,date0 =run_paper(filt_sd.filtered, #I think?\n",
    "#                             sd_weather,gap=gap,delta=[1,1,1],fit=True,s=0.98)\n",
    "\n",
    "# vid = 2 # index of autocorrelation\n",
    "# sm = FF.get('sm')[vid,:] # mean of autocorrelation\n",
    "# sC = FF.get('sC')[vid,vid,:] # variance of autocorrelation\n",
    "# snu = FF.get('snu') # degree of freedom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ab0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "delta_finding-Copy2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
